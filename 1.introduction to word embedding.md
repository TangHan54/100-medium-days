### Article: Introduction to Word Embeddings

Link: https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc

1. what is word embedding and why do we need it?
Word embedding is to transform natural language to numeric form so that we can apply mathematical rules and matrix operation to them.

2. Different encoding methods

    - One-hot encoding (count vectorizing) 
        - m dimension. 
    m: number of unique words.

    - TF-IDF
        - term frequency * inverse document frequency
        - Common words has less significance, important words might appear less frequent.

    - Co-Occurrence matrix
        - matrix with m*m dimension.m: number of unique words.

- Neural probabilistic model

    Initialize the embedding to random values with pre-defined representation size. Based on some NLP task to train the neural network and modify the embedding of text.
    
    Both trained for the NLP task and learned the embedding.

    - word2vec
        - Continuous Bag-of-words(CBOW) approach \
            Learn embedding by predicting current word based on the surrounding words.

        - Continuous Skip-Gram \
            Learn embedding by predicting surrounding words based on the current word.

        Both approaches use local word usage context (with a pre-defined window of neighboring words).The larger the window is, the more topical similarities that are learned by the embedding. Forcing a smaller window results in more semantic, syntactic, and functional similarities to be learned.

        More effecient and less dimensions.

    - GloVe
        
        extension of word2vec.

        GloVeâ€™s contribution was the addition of global statistics in the language modeling task to generate the embedding. There is no window feature for local context. Instead, there is a word-context/word co-occurrence matrix that learns statistics across the entire corpora.

    - FastText

        incorporated sub-word information. split all words into a bag of n-gram characters. support out-of-vocabulary words.

        It uses skip-gram objective with negative sampling. meaning All sub-words are positive examples, and then random samples from a dictionary of words in the corpora are used as negative examples. 

    - Poincare Embedding(Hierarchal Representation)

        By placing their embedding into a hyperbolic space, they can use properties of hyperbolic space to use distance to encode similarity and the norm of vectors to encode hierarchal relationships.

    - ELMo
        
        contextual word vectors. The representations are generated from a function of the entire sentence to create word-level representations. The embeddings are generated at a character-level, so they can capitalize on sub-word units like FastText and do not suffer from the issue of out-of-vocabulary words.

        ELMo is trained as a bi-directional, two layer LSTM language model.

    - Probabilistic FastText
        
        words are represented as Gaussian mixture models.


